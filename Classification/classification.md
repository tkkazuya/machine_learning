# Classification

1. 分類概要
2. 決定木の実装
3. 代表的な分類のアルゴリズム
4. 評価指標

---

## 1.分類
**分類**：カテゴリ（離散値）を予測すること

分類とは、（二次元の場合は）「カテゴリが異なる複数のデータを見分けることができる境界線を求めること」
- 一本の直線で分けられることを**線形分離可能**、そのアルゴリズムは**線形分類器**と呼ばれる。
- 直線を複数組み合わせて分類するとき、そのアルゴリズムを**非線形分類器**と呼ぶ。


**線形分類器の例**
- 単純パーセプトロン
- 線形SVM
- ロジスティック回帰

**非線形分類器の例**
- k-近傍法
- 決定木（分類木）
- ランダムフォレスト
- 非線形SVM
- NN（ニューラルネットワーク）

---
## 2.決定木の理論

条件分岐を繰り返すことによって、分類を行う。
決定 機では、各ノードでデータの「不純度」が低くなるように2つのグループに分割する。

**ジニ不純度（Gini's Impurity）**
- cfジニ係数：富の分配率を表す指数
- 分岐されたノードの不純度を表す

scikit-learnでは、CARTというアルゴリズムが使用されている
```
sklearn.tree.DecisionTreeClassifier
```

### 決定木（非線形分類器の例）

--- 
## 3.代表的な分類のアルゴリズム

### SVM（どちらも？）
- 教師ありの分類器の人る
- イメージは（パーセプトロン）＋（マージン最大化）＋（カーネル法）
- 深層学習登場までは最強のアルゴリズムだったぽい
- 境界線に最も近いサンプルとの距離（マージン）を最大化するように境界線を定義

- **カーネルトリック**を用いることによって、非線形分類が可能

**カーネルトリック**
カーネル関数を用いて高次元の特徴空間へデータを写像し、特徴空間上で線形分離を行う手法
ex:2次元のデータを2次元へ写像し、平面で線形分類



### ロジスティック回帰（線形分類器の例）
確率を出力する


---
## 4.評価指標

混同行列（Confusion Matrix）
- TP (True Positive、真陽性)：を正例として、その予測が正しい場合の数

- FP (False Positive、偽陽性)：予測値を正例として、その予測が誤りの場合の数

- TN (True Negative、真陰性)：予測値を負例として、その予測が正しい場合の数

- FN (False Negative、偽陰性)：予測値を負例として、その予測が誤りの場合の数

評価指標の種類
1.Accuracy（正解率）
- 全ての判定のうち、予測が正しい割合
- 分類の精度指標として最も一般的

$$
Accuracy = \frac{TP+TN}{TP+FP+TN+FN} 
$$

2. Precision（適合率）

- 予測したもののうち、その予測が正しい割合
- 誤診を少なくしたいときに使用
$$
Precision = \frac{TP}{TP+FP}
$$

3. Recall（再現率）

- 正例のうち、正しく予測がされた割合
- 正例の見逃しを避けたいときに使用
$$
Recall = \frac{TP}{TP+FN}
$$
4. F1Score（F値）
- トレードオフの関係のPrecision・Recallのバランスをとるための指標
- Precision・Recallの調和平均の形で表される。


